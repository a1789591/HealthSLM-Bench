<!DOCTYPE html>
<html lang="en">
<head>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta charset="UTF-8">
<title>HealthSLM-Bench</title>

<style>
    body {
        margin: 0;
        padding: 0;
        background: #f2f4f7;
        /* background: #ffffff; */
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
        counter-reset: section;
    }

    .container {
        max-width: 900px;
        margin: 40px auto;
        background: #ffffff;
        padding: 40px 50px;
        border-radius: 16px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.06);
    }

    h1 {
        margin-bottom: 6px;
        font-size: 2rem;
        line-height: 1.3;
    }
    
    h2{
        counter-reset: subsection;
    }
    
    h2::before {
        counter-increment: section;
        content: counter(section) ". ";
    }

    h3::before {
        counter-increment: subsection;
        content: counter(section) "." counter(subsection) " ";
    }
    
    .no-number::before {
        content: "" !important;
        counter-increment: none !important;
    }
    
    .center{
        text-align:center;
    }

    .subtitle {
        font-size: 1.25rem;
        color: #374151;
        margin-bottom: 16px;
    }

    .authors {
        font-size: 1.05rem;
        margin-bottom: 4px;
        text-align: center;
    }

    .affiliations {
        color: #555;
        font-size: 0.95rem;
        margin-bottom: 14px;
        line-height: 1.5;
        text-align: center;
    }

    .meta {
        color: #555;
        font-size: 0.95rem;
        margin-bottom: 20px;
    }

    hr {
        border: none;
        border-top: 1px solid #e5e7eb;
        margin: 30px 0;
    }

    .bibtex-block {
        background: #000000;
        color: #e2e8f0;
        /* padding: 20px 24px; */
        border-radius: 10px;
        margin-top: 16px;
        box-shadow: 0 6px 16px rgba(0,0,0,0.15);
        position: relative;
        overflow-x: auto;
        width: 100%;
        margin:24px 0;
    }

    .bibtex-block pre {
        /* display: inline-block; */
        /* margin: 0; */
        font-family: Menlo, Monaco, "Courier New", monospace;
        font-size: 14px;
        white-space: pre;
    }

    .bibtex-content {
        margin-left: 7px;
    }

    .bib-copy-btn {
        position: absolute;
        top: 14px;
        right: 14px;
        background: #e2e8f0;
        color: #0f172a;
        border: none;
        padding: 6px 12px;
        border-radius: 12px;
        font-size: 12px;
        cursor: pointer;
        font-weight: 500;
    }

    .bib-copy-btn:hover {
        background: #cbd5e1;
    }

    .btn-bar {
        display: flex;
        gap: 20px;
        margin: 30px 0;
        justify-content: center;
    }
    
    .btn {
        display: flex;
        align-items: center;
        gap: 10px;
        background: #1e2532;
        color: white;
        padding: 14px 26px;
        border-radius: 18px;
        text-decoration: none;
        font-size: 1.1rem;
        font-weight: 500;
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        transition: transform 0.15s ease, box-shadow 0.15s ease;
    }
    
    .btn img {
        width: 22px;
        height: 22px;
        filter: brightness(0) invert(1);
    }
    
    .btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(0,0,0,0.25);
    }

    .image-box {
        margin: 30px 0;
        text-align: center;
        padding: 0px;
        background: #ffffff; 
        border-radius: 12px;   
    }

    .no-invert {
        filter: none !important;
    }

    .image-box img {
        width: 100%;
        border-radius: 12px;
        align-items: center;
        /* box-shadow: 0 2px 10px rgba(0,0,0,0.08); */
    }
    .image-row {
        display: flex;
        flex-direction: row;
        align-items: center;
        gap: 20px;
        /* margin: 30px 0; */
        justify-content: center;
        width: 100%;
        border-radius: 12px;
    }

    .left-img {
        width: 50%;
    }

    .right-col {
        width: 50%;
    }

    .right-col img {
        width: 100%;
        margin-bottom: 10px; 
    }
    ul li {
        margin-bottom: 8px;
    }
    .config-table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
    font-size: 1rem;
}

    .config-table td {
        padding: 10px 12px;
        border-bottom: 1px solid #e5e7eb;
    }

    .config-table tr td:first-child {
        width: 40%;
        font-weight: 600;
    }

    .config-card {
    background: #f9fafb;
    padding: 20px 24px;
    border-radius: 12px;
    box-shadow: 0 2px 6px rgba(0,0,0,0.06);
    margin: 20px 0;
}

    .config-card ul {
        margin: 0;
        padding-left: 20px;
    }

    .config-card li {
        margin-bottom: 6px;
    }


    .datasets a {
        text-decoration: none; 
        color: #000000;        
        font-weight: 600;       
        font-size: 18px;
        cursor: pointer;        
    }

    .datasets a:hover {
        text-decoration: underline;  /* 可选：鼠标悬停时下划线 */
    }
        /* .image-row .image-box:first-child img {
    margin-top: 25%; 
} */
</style>
</head>

<body>

<div class="container">

<h1 class="center">HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring</h1>

<div class="authors">
    <span>Xin Wang<sup>1</sup>,</span>
    <span>Ting Dang<sup>1</sup>,</span>
    <span>Xinyu Zhang<sup>2</sup>,</span>
    <span>Vassilis Kostakos<sup>1</sup>,</span>
    <span>Michael Witbrock<sup>2</sup>,</span>
    <span>Hong Jia<sup>2</sup></span>
</div>

<div class="affiliations">
    <span><sup>1</sup> University of Melbourne, Australia ·</span>
    <span><sup>2</sup> University of Auckland, New Zeland</span>
</div>

<div class="btn-bar center">

    <a class="btn" href="https://arxiv.org/abs/2509.07260">
        <img class="no-invert" src="images/arxiv3.png">
        Paper
    </a>

    <a class="btn" href="https://github.com/a1789591/HealthSLM-Bench">
        <img src="https://cdn-icons-png.flaticon.com/512/25/25231.png">
        Code
    </a>

</div>

<hr>

<h2 class="no-number">Abstract</h2>

<p>
    Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals’ quality of life. 
    Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. 
    However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. 
    To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. 
    Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. 
    We systematically benchmarked SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. 
    However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.
</p>

<div class="image-box">
    <img src="images/overview.png" style="width: 80%;align-items: center;" alt="Overview figure">
</div>

<h2>Overview</h2>
<p> Our <strong>HealthSLM-Bench</strong> is built upon two main components: Health-Domain Adaptation and Edge-Device Effciency Evaluation. 
    In the Health-Domain Adaptation stage, we format the wearable health data into structured prompts and evaluate the performance of SLMs under different level of apadation strategies, 
    including zero-shot, few-shot, and instruction-tuning. After adaptation, fine-tuned SLMs are compressed into 4-bit GGUF formats and deployed on mobile devices for on-device efficiency evaluation. 
    </p>

    <p>Together, this benchmark enables a comprehensive evaluation of SLMs' performance in on-device healthcare monitoring, where 14-day wearable signals are converted into natural-language inputs and fed into SLMs for real-time prediction of stress, sleep quality, 
    anxiety, fatigue, activity, calories, and other personalized physiological metrics. </p>

<!-- wearable sensor signals and self-reported measures are transformed into structured prompts -->
<p>Our main contributions in this work are as follows: </p>
<ul>
    <li>We introduce HealthSLM-Bench, an extensive benchmark that systematically evaluates nine
        SOTA SLMs on eight health prediction tasks across three real-world mobile and wearable datasets.
    </li>
    <li>We investigate various evaluation paradigms, including zero-shot, few-shot, and instruction-based fine-tuning, 
        providing a comprehensive performance analysis under different adaptation scenarios.
    </li>
    <li> We demonstrate the feasibility of deploying fine-tuned SLMs on resource-constrained mobile devices and quantify their efficiency in terms of real-world memory and latency footprints.
    </li>
</ul>

<h2>Datasets</h2>
<p>We used three health wearable sensor datasets to evaluate the performance of SLMs on health prediction tasks:
        <a class="datasets" href="https://dl.acm.org/doi/10.1145/3339825.3394926">PMData</a>,
        <a class="datasets" href="https://dl.acm.org/doi/10.1145/3569485">GLOBEM</a>,
        and <a class="datasets" href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZS2Z2J">AW_FB</a>. The details of each dataset are as follows:
</p>

<div class="image-box">
    <img src="images/datasets.png" alt="Datasets figure">
</div>

<p>In experiments, we formatted wearable data into prompts (\( x \)) and input to SLM for inference to obtain predicted health status(\( \hat{y} \)),
     which is then compared with self-report ground-truth health label(\( y \)). The extracted task label distributions of each dataset are as follows:
</p>

<div class="image-row">
    <div class="image-box" style="width: 50%;">
        <img class="left-img" style="width: 100%; align-items: center;" src="images/data1.png" alt="Dataset 1 figure">
    </div>
    <div class="right-col">
        <img style="width: 100%; align-items: center;" src="images/data2.png" alt="Dataset 2 figure">
        <img style="width: 100%; align-items: center;" src="images/data3.png" alt="Dataset 3 figure">
    </div>
</div>

<h2>Models and Training Details</h2>
<p>In HealthSLM-Bench, we assessed 9 most state-of-the-art SLMs between 1 to 4B from top-tier tech companies, 
including Google’s Gemma2-2B-it, Microsoft’s Phi-3-mini-4k, HuggingFace’s SmolLM1.7B, Alibaba’s Qwen2-1.5B, TinyLlama’s TinyLlama-1.1B, and
Meta-Llama’s Llama-3.2-1B and their variants.
All models are used in their instruction-tuned version to ensure consistent behavior under
zero-shot, few-shot, and supervised fine-tuning settings. 
</p>

<!-- <p>The training details are as follows: </p> -->
<p>During training, all SLMs are fine-tuned using LoRA-based parameter-efficient training on an NVIDIA A100 80GB GPU,
trained for 3 epochs with a batch size of 128. We used the Adam optimizer and the learning rate is set to 5e-5 with cosine decay and 5% warmup. 
Training all nine models end-to-end takes roughly 7 hours.</p>

<h3>Training Configuration</h3>

<div class="config-card">
    <ul>
        <li><strong>GPU:</strong> NVIDIA A100 80GB</li>
        <li><strong>Batch Size:</strong> 128</li>
        <li><strong>Epochs:</strong> 3</li>
        <li><strong>Optimizer:</strong> Adam</li>
        <li><strong>Learning Rate:</strong> 5e-5</li>
        <li><strong>Warmup:</strong> 5%</li>
        <li><strong>Decoding:</strong> Greedy</li>
        <li><strong>Max Tokens:</strong> 30</li>
    </ul>
    </div>
<!-- <ul>
    <li>preprocessing </li>

</ul> -->

<h2>On-device deployment</h2>
<p>After training, we deploy the top-performing health domain-adapted SLMs on an iPhone 15 Pro Max equipped with 8 GB of RAM 
to evaluate their real-world on-device efficiency. As shown in the Figure 3, the mobile depoly pipeline for health domain-adapted SLMs includes two main steps: conversion to the GGUF format and 4-bit quantization.
The GGUF format (Generalized Graphical Unified Format) ensures compatibility with lightweight inference engines on mobile devices such as Llama.cpp, 
while 4-bit quantization enables efficient deployment on resource-constrained mobile devices. 
Build upon this, we developed an iOS application, <strong>HealthAI@Unimelb</strong>, to support our on-device efficiency evaluation experiments.</p>


<div class="image-row">
    <div class="image-box" style="margin: 0;">
        <img style="width: 92%; align-items: center;" src="images/on-device-deployment.png" alt="On-device deployment figure">
    </div>

    <div class="image-box" style="margin: 0;">
        <img style="width: 80%; align-items: center;" src="images/phone_demo1.png" alt="iPhone demo figure">
    </div>
</div>

<h2>Results</h2>
    <!-- <div class="image-row"> -->
        <div class="image-box">
            <img style="width: 80%; align-items: center;" src="images/result1.1.png" alt="Zero-shot figure">
        </div>
        <div class="image-box">
            <img style="width: 80%; align-items: center;" src="images/result1.2.png" alt="Zero-shot figure">
        </div>
    <!-- </div> -->

<!-- <div class="image-row" style="width: 80%;"> -->
    <div class="image-box">
        <img style="width: 80%; align-items: center;" src="images/result2.1.png" alt="Few-shot figure">
    </div>
    <div class="image-box">
        <img style="width: 80%; align-items: center;" src="images/result2.2.png" alt="Few-shot figure">
    </div>
<!-- </div> -->


<div class="image-box">
    <img style="width: 80%; align-items: center;" src="images/result3.1.png" alt="Instruction-tuning figure">
</div>

<div class="image-box">
    <img style="width: 50%; align-items: center;" src="images/result3.2.png" alt="Instruction-tuning figure">
</div>
<div class="image-box">
    <img style="width: 80%; align-items: center;" src="images/result3.3.png" alt="Instruction-tuning figure">
</div>

<div class="image-box">
    <img src="images/result4.1.png" alt="Effciency evaluation figure">
</div>

<h2>Reproducibility</h2>
<p>The relevant code and instructions to reproduce this work are available at the <a href="https://github.com/a1789591/HealthSLM-Bench">github repository</a>.</p>

<h2 class="no-number">BibTeX</h2>

<p>If you find <strong>HealthSLM-Bench</strong> useful in your research, please cite it as:</p>

<div class="bibtex-block">
<pre class="bibtex-content">
@article{wang2025healthslm,
  title   = {HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring},
  author  = {Wang, Xin and Dang, Ting and Zhang, Xinyu and Kostakos, Vassilis and Witbrock, Michael J. and Jia, Hong},
  journal = {arXiv preprint arXiv:2509.07260},
  year    = {2025}
}
</pre>
</div>

</div>

</body>
</html>