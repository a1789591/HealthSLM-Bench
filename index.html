<!DOCTYPE html>
<html lang="en">
<head>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<meta charset="UTF-8">
<title>HealthSLM-Bench</title>

<style>
    body {
        margin: 0;
        padding: 0;
        background: #f2f4f7;
        /* background: #ffffff; */
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
        counter-reset: section;
    }

    .container {
        max-width: 900px;
        margin: 40px auto;
        background: #ffffff;
        padding: 40px 50px;
        border-radius: 16px;
        box-shadow: 0 4px 15px rgba(0,0,0,0.06);
    }

    h1 {
        margin-bottom: 6px;
        font-size: 2rem;
        line-height: 1.3;
    }
    
    h2{
        counter-reset: subsection;
    }
    
    h2::before {
        counter-increment: section;
        content: counter(section) ". ";
    }

    h3::before {
        counter-increment: subsection;
        content: counter(section) "." counter(subsection) " ";
    }
    
    .no-number::before {
        content: "" !important;
        counter-increment: none !important;
    }
    
    .center{
        text-align:center;
    }

    .subtitle {
        font-size: 1.25rem;
        color: #374151;
        margin-bottom: 16px;
    }

    .authors {
        font-size: 1.05rem;
        margin-bottom: 4px;
        text-align: center;
    }

    .affiliations {
        color: #555;
        font-size: 0.95rem;
        margin-bottom: 14px;
        line-height: 1.5;
        text-align: center;
    }

    .meta {
        color: #555;
        font-size: 0.95rem;
        margin-bottom: 20px;
    }

    hr {
        border: none;
        border-top: 1px solid #e5e7eb;
        margin: 30px 0;
    }

    .bibtex-block {
        background: #f2f4f7;
        border: 1px solid #e5e7eb;
        color: #1f2937;
        /* padding: 20px 24px; */
        border-radius: 10px;
        margin-top: 16px;
        box-shadow: 0 6px 16px rgba(0,0,0,0.15);
        position: relative;
        overflow-x: auto;
        width: 100%;
        margin:24px 0;
    }


    .bibtex-block pre {
        /* display: inline-block; */
        /* margin: 0; */
        font-family: Menlo, Monaco, "Courier New", monospace;
        /* font-family: "JetBrains Mono", "Fira Code", monospace; */
        font-size: 14px;
        white-space: pre;
    }

    .bibtex-content {
        margin-left: 7px;
    }

    .bib-copy-btn {
        position: absolute;
        top: 14px;
        right: 14px;
        background: #e2e8f0;
        color: #0f172a;
        border: none;
        padding: 6px 12px;
        border-radius: 12px;
        font-size: 12px;
        cursor: pointer;
        font-weight: 500;
    }

    .bib-copy-btn:hover {
        background: #cbd5e1;
    }

    .btn-bar {
        display: flex;
        gap: 20px;
        margin: 30px 0;
        justify-content: center;
    }
    
    .btn {
        display: flex;
        align-items: center;
        gap: 10px;
        background: #1e2532;
        color: white;
        padding: 14px 26px;
        border-radius: 18px;
        text-decoration: none;
        font-size: 1.1rem;
        font-weight: 500;
        box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        transition: transform 0.15s ease, box-shadow 0.15s ease;
    }
    
    .btn img {
        width: 22px;
        height: 22px;
        filter: brightness(0) invert(1);
    }
    
    .btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 16px rgba(0,0,0,0.25);
    }

    .image-box {
        margin: 30px 0;
        text-align: center;
        padding: 0px;
        background: #ffffff; 
        border-radius: 12px;   
    }

    .no-invert {
        filter: none !important;
    }

    .image-box img {
        width: 100%;
        border-radius: 12px;
        align-items: center;
        /* box-shadow: 0 2px 10px rgba(0,0,0,0.08); */
    }
    .image-row {
        display: flex;
        flex-direction: row;
        align-items: center;
        gap: 20px;
        /* margin: 30px 0; */
        justify-content: center;
        width: 100%;
        border-radius: 12px;
    }

    .left-img {
        width: 50%;
    }

    .right-col {
        width: 50%;
    }

    .right-col img {
        width: 100%;
        margin-bottom: 10px; 
    }
    ul li {
        margin-bottom: 8px;
    }
    .config-table {
    width: 100%;
    border-collapse: collapse;
    margin: 20px 0;
    font-size: 1rem;
}

    .config-table td {
        padding: 10px 12px;
        border-bottom: 1px solid #e5e7eb;
    }

    .config-table tr td:first-child {
        width: 40%;
        font-weight: 600;
    }

    .config-card {
    background: #f9fafb;
    padding: 20px 24px;
    border-radius: 12px;
    box-shadow: 0 2px 6px rgba(0,0,0,0.06);
    margin: 20px 0;
}

    .config-card ul {
        margin: 0;
        padding-left: 20px;
    }

    .config-card li {
        margin-bottom: 6px;
    }


    .formatted_link a {
        text-decoration: none; 
        color: #000000;        
        font-weight: 600;       
        font-size: 18px;
        cursor: pointer;        
    }

    .formatted_link a:hover {
        text-decoration: underline;
    }

    .model-card {
    background: #f7f9fb;
    padding: 14px 16px;
    margin-bottom: 12px;
    border-radius: 10px;
    border-left: 4px solid #4a90e2;
    }

    .model-card a {
    text-decoration: none;
    font-weight: 600;
    color: #111;
    }

    .model-card a:hover {
    text-decoration: underline;
    }

    .config-table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 12px;
    }

    .config-table td {
    border-bottom: 1px solid #e5e7eb;
    padding: 10px 8px;
    font-size: 16px;
    }

    .config-table tr td:first-child {
    font-weight: 600;
    width: 200px;
    }
        /* .image-row .image-box:first-child img {
    margin-top: 25%; 
} */
</style>
</head>

<body>

<div class="container">

<h1 class="center">HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring</h1>

<div class="authors">
    <span>Xin Wang<sup>1</sup>,</span>
    <span>Ting Dang<sup>1</sup>,</span>
    <span>Xinyu Zhang<sup>2</sup>,</span>
    <span>Vassilis Kostakos<sup>1</sup>,</span>
    <span>Michael Witbrock<sup>2</sup>,</span>
    <span>Hong Jia<sup>2</sup></span>
</div>

<div class="affiliations">
    <span><sup>1</sup> University of Melbourne, Australia ·</span>
    <span><sup>2</sup> University of Auckland, New Zeland</span>
</div>

<div class="btn-bar center">

    <a class="btn" href="https://arxiv.org/abs/2509.07260">
        <img class="no-invert" src="images/arxiv3.png">
        Paper
    </a>

    <a class="btn" href="https://github.com/a1789591/HealthSLM-Bench">
        <img src="https://cdn-icons-png.flaticon.com/512/25/25231.png">
        Code
    </a>

</div>

<hr>

<h2 class="no-number">Abstract</h2>

<p>
    Mobile and wearable healthcare monitoring play a vital role in facilitating timely interventions, managing chronic health conditions, and ultimately improving individuals’ quality of life. 
    Previous studies on large language models (LLMs) have highlighted their impressive generalization abilities and effectiveness in healthcare prediction tasks. 
    However, most LLM-based healthcare solutions are cloud-based, which raises significant privacy concerns and results in increased memory usage and latency. 
    To address these challenges, there is growing interest in compact models, Small Language Models (SLMs), which are lightweight and designed to run locally and efficiently on mobile and wearable devices. 
    Nevertheless, how well these models perform in healthcare prediction remains largely unexplored. 
    We systematically benchmarked SLMs on health prediction tasks using zero-shot, few-shot, and instruction fine-tuning approaches, and deployed the best performing fine-tuned SLMs on mobile devices to evaluate their real-world efficiency and predictive performance in practical healthcare scenarios. Our results show that SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. 
    However, challenges remain, particularly in handling class imbalance and few-shot scenarios. These findings highlight SLMs, though imperfect in their current form, as a promising solution for next-generation, privacy-preserving healthcare monitoring.
</p>

<div class="image-box">
    <img src="images/overview.png" style="width: 80%;align-items: center;" alt="Overview figure">
</div>

<h2>Overview</h2>
<p> Our <strong>HealthSLM-Bench</strong> is built upon two main components: Health-Domain Adaptation and Edge-Device Effciency Evaluation. 
    In the Health-Domain Adaptation stage, we format the wearable health data into structured prompts and evaluate the performance of SLMs under different level of apadation strategies, 
    including zero-shot, few-shot, and instruction-tuning. After adaptation, fine-tuned SLMs are compressed into 4-bit GGUF formats and deployed on mobile devices for on-device efficiency evaluation. 
    </p>

    <p>Together, this benchmark enables a comprehensive evaluation of SLMs' performance in on-device healthcare monitoring, where 14-day wearable signals are converted into natural-language inputs and fed into SLMs for real-time prediction of stress, sleep quality, 
    anxiety, fatigue, activity, calories, and other personalized physiological metrics. </p>

<!-- wearable sensor signals and self-reported measures are transformed into structured prompts -->
<p>Our main contributions in this work are as follows: </p>
<ul>
    <li>We introduce HealthSLM-Bench, an extensive benchmark that systematically evaluates nine
        SOTA SLMs on eight health prediction tasks across three real-world mobile and wearable datasets.
    </li>
    <li>We investigate various evaluation paradigms, including zero-shot, few-shot, and instruction-based fine-tuning, 
        providing a comprehensive performance analysis under different adaptation scenarios.
    </li>
    <li> We demonstrate the feasibility of deploying fine-tuned SLMs on resource-constrained mobile devices and quantify their efficiency in terms of real-world memory and latency footprints.
    </li>
</ul>

<h2>Datasets</h2>
<p class="formatted_link">We used three health wearable sensor datasets to evaluate the performance of SLMs on health prediction tasks:
        <a href="https://dl.acm.org/doi/10.1145/3339825.3394926">PMData</a>,
        <a href="https://dl.acm.org/doi/10.1145/3569485">GLOBEM</a>,
        and <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ZS2Z2J">AW_FB</a>. The details of each dataset are as follows:
</p>

<div class="image-box">
    <img src="images/datasets.png" alt="Datasets figure">
</div>

<p>In experiments, we formatted wearable data into prompts (\( x \)) and input to SLM for inference to obtain predicted health status(\( \hat{y} \)),
     which is then compared with self-report ground-truth health label(\( y \)). The extracted task label distributions of each dataset are as follows:
</p>

<div class="image-row">
    <div class="image-box" style="width: 50%;">
        <img class="left-img" style="width: 100%; align-items: center;" src="images/data1.png" alt="Dataset 1 figure">
    </div>
    <div class="right-col">
        <img style="width: 100%; align-items: center;" src="images/data2.png" alt="Dataset 2 figure">
        <img style="width: 100%; align-items: center;" src="images/data3.png" alt="Dataset 3 figure">
    </div>
</div>

<h2>Models</h2>
<p class="formatted_link">In HealthSLM-Bench, we assessed 9 most state-of-the-art SLMs between 1 to 4B from top-tier tech companies, 
including Google’s Gemma2-2B-it, Microsoft’s Phi-3-mini-4k, HuggingFace’s SmolLM1.7B, Alibaba’s Qwen2-1.5B, TinyLlama’s TinyLlama-1.1B, Meta-Llama’s Llama-3.2-1B and Llama-3.2-3B. 
and their variants. The model details are as follows:

<ul class="formatted_link">
    <li>
      <strong>
        <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct" target="_blank" rel="noopener noreferrer">
          Phi-3-mini-4k-Instruct </a> </strong>:
      Microsoft’s smallest Phi-3 model (3.8B params), trained on a mixture of synthetic and curated web data,
      with strong reasoning-dense behavior.
    </li>
  
    <li>
      <strong>
        <a href="https://huggingface.co/microsoft/Phi-3.5-mini-instruct" target="_blank" rel="noopener noreferrer">
          Phi-3.5-mini-Instruct </a> </strong>:
      An upgraded version of Phi-3-mini-4k-Instruct, trained on more reasoning-dense data for better
      instruction alignment and multi-step reasoning.
    </li>
  
    <li>
      <strong>
        <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0" target="_blank" rel="noopener noreferrer">
          TinyLlama-1.1B-Chat-v1.0 </a> </strong>:
      A distilled 1.1B-parameter version of LLaMA-2, trained on UltraChat-style data to provide a compact
      chat model suitable for a wide range of tasks.
    </li>  
    <li>
      <strong>
        <a href="https://huggingface.co/HuggingFaceTB/SmolLM-1.7B-Instruct" target="_blank" rel="noopener noreferrer">
          SmolLM-1.7B-Instruct </a> </strong>:
      HuggingFace’s 1.7B-parameter SLM trained on the SmolLM-Corpus (textbooks, stories, and educational data),
      targeting general instruction following.
    </li>
  
    <li>
      <strong>
        <a href="https://huggingface.co/Qwen/Qwen2-1.5B-Instruct" target="_blank" rel="noopener noreferrer">
          Qwen2-1.5B-Instruct </a> </strong>:
      Alibaba’s 1.5B-parameter SOTA SLM in the Qwen2 family, trained on diverse instruction-following data,
      including coding and math-oriented tasks.
    </li>  
    <li>
      <strong>
        <a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct" target="_blank" rel="noopener noreferrer">
          Llama-3.2-1B-Instruct </a> </strong>:
      Meta’s 1B-parameter Llama-3.2 instruction-tuned model, optimized for efficient on-device language tasks.
    </li>
  
    <li>
      <strong>
        <a href="https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct" target="_blank" rel="noopener noreferrer">
          Llama-3.2-3B-Instruct </a> </strong>:
      The 3B-parameter Llama-3.2 instruction model, offering stronger reasoning while remaining compact
      enough for resource-constrained deployment.
    </li>
  </ul>

<p>All models are used in their instruction-tuned version to ensure consistent behavior under zero-shot, few-shot, and supervised fine-tuning settings. </p> 

<h2>Training Configuration</h2>
<p>During training, all SLMs are fine-tuned using LoRA-based parameter-efficient training on an NVIDIA A100 80GB GPU,
    trained for 3 epochs with a batch size of 128. In specific, we used the AdamW optimizer and the learning rate is set to 5e-5 with cosine decay and 5% warmup. 
    Training all nine models end-to-end takes roughly 7 hours.</p>
<p>The training details are as follows: </p>

<table class="config-table">
  <tr><td>GPU</td><td>NVIDIA A100 80GB</td></tr>
  <tr><td>Batch Size</td><td>128</td></tr>
  <tr><td>Epochs</td><td>3</td></tr>
  <tr><td>Optimizer</td><td>AdamW</td></tr>
  <tr><td>Learning Rate</td><td>5e-5 (cosine decay)</td></tr>
  <tr><td>Warmup</td><td>5% of total training steps</td></tr>
  <tr><td>Decoding</td><td>Greedy</td></tr>
  <tr><td>Max Tokens Generated</td><td>30 tokens</td></tr>
</table>


<h2>On-device deployment</h2>
<p>After training, we deploy the top-performing health domain-adapted SLMs on an iPhone 15 Pro Max equipped with 8 GB of RAM 
to evaluate their real-world on-device efficiency.</p>

<div class="image-row">
    <div class="image-box" style="margin: 0;">
        <img style="width: 92%; align-items: center;" src="images/on-device-deployment.png" alt="On-device deployment figure">
    </div>

    <div class="image-box" style="margin: 0;">
        <img style="width: 80%; align-items: center;" src="images/phone_demo1.png" alt="iPhone demo figure">
    </div>
</div>

<p> As shown in the Figure 3, the mobile depoly pipeline for health domain-adapted SLMs includes two main steps: conversion to the GGUF format and 4-bit quantization.
    The GGUF format (Generalized Graphical Unified Format) ensures compatibility with lightweight inference engines on mobile devices such as Llama.cpp, 
    while 4-bit quantization enables efficient deployment on resource-constrained mobile devices. 
    Build upon this, we developed an iOS application, <strong>HealthAI@Unimelb</strong>, to support our on-device efficiency evaluation experiments.</p>

<h2>Results</h2>
<p>In our experiments, we firstly evaluate the performance of SLMs in comparison with prior SOTA LLM baselines on the same datasets, under different level of adapation settings, including zero-shot, few-shot, and instruction-tuning evaluation paradigms. 
    The results are presented below:</p>

    <h3> Zero-shot</h3>
        <div class="image-box">
            <img style="width: 80%; align-items: center;" src="images/result1.1.png" alt="Zero-shot figure">
        </div>
        <div class="image-box">
            <img style="width: 80%; align-items: center;" src="images/result1.2.png" alt="Zero-shot figure">
        </div>
    <p> Under zero-shot settings, SLMs generally match or surpass LLMs on most health prediction
        tasks, notably achieving better results in stress, readiness, and fatigue predictions. Surpringly, some leading SLMs,
        such as Gemma-2-2B-it and Phi-3-mini-4k, show consistent strength compared with SOTA LLMs. </p>
    
    <h3> Few-shot</h3>
    <div class="image-box">
        <img style="width: 80%; align-items: center;" src="images/result2.5.png" alt="Few-shot figure">
    </div>

    <p>Under few-shot settings, providing even a small number of examples leads to noticeable performance improvements over the zero-shot setting. 
       Compared with LLMs, SLMs remain competitive in few-shot healthcare tasks, even when only a single example is given. </p>
       <!-- (see the full few-shot results in the paper for additional details) -->

    <p>However, we also observe a collapse phenomenon when the number of provided samples is very low, where the model tends to converge to a fixed and uninformative prediction. 
       On our further investigation, we found that increasing the number of examples can counter this effect and yields more stable and reliable predictions. </p>

    <div class="image-box">
        <img style="width: 80%; align-items: center;" src="images/result2.2.png" alt="Few-shot figure">
    </div>
    
    <h3> Instruction-tuning</h3>
    <div class="image-box">
        <img style="width: 80%; align-items: center;" src="images/result3.1.png" alt="Instruction-tuning figure">
    </div>

    <p> The instruction-tuning results show that the health domain-adapted SLMs can achieve performance comparable to LLMs while offering substantial gains in efficiency and privacy. </p>
    <div class="image-box">
        <img style="width: 50%; align-items: center;" src="images/result3.2.png" alt="Instruction-tuning figure">
    </div>

    <p> We also observed that the SLMs tend to getting stuck at suboptimal performance on tasks from class-imbalanced datasets, where the model tends to predict only the majority classes. </p>
    <div class="image-box">
        <img style="width: 80%; align-items: center;" src="images/result3.3.png" alt="Instruction-tuning figure">
    </div>

    <p>To tackle the class-imbalance issue, we also run additional experiments with data augmentation strategies to balance the training datasets. 
        (Please refer to the Appendix section in the paper for full details) </p>


    <h3> On-device efficiency evaluation</h3>
    <p> After performance evaluation, we ran inference with the two top-performing instructional-tuned SLMs, Phi-3-mini-4k and TinyLlama-1.1B, on an iPhone 15 Pro Max using the iOS application (HealthAI@Unimelb) to evaluate their 
        real-world on-device efficiency.</p>
    <div class="image-box">
        <img src="images/result4.1.png" alt="Effciency evaluation figure">
    </div>
    <p> Our on-device efficiency evaluation results show that SLMs achieve substantial reductions in both input processing latency and generation latency. 
        Indicating them as ideal and practical solutions for resource-constrained mobile health applications. </p>

<h2>Reproducibility</h2>
<p>The relevant code and instructions to reproduce this work has been released at the <a href="https://github.com/a1789591/HealthSLM-Bench">GitHub repository</a>.
    The iOS application (HealthAI@Unimelb) will be made public shortly.</p>

<h2 class="no-number">BibTeX</h2>

<p>If you find <strong>HealthSLM-Bench</strong> useful in your research, please cite it as:</p>

<div class="bibtex-block">
<pre class="bibtex-content">
@article{wang2025healthslm,
  title   = {HealthSLM-Bench: Benchmarking Small Language Models for Mobile and Wearable Healthcare Monitoring},
  author  = {Wang, Xin and Dang, Ting and Zhang, Xinyu and Kostakos, Vassilis and Witbrock, Michael J. and Jia, Hong},
  journal = {arXiv preprint arXiv:2509.07260},
  year    = {2025}
}
</pre>
</div>

</div>

</body>
</html>